{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis using below two technique\n",
    "1. VADER (Valence Aware Dictionary and sEntiment Reasoner) - Bag of words approach\n",
    "2. Roberta Pretrained Model from ðŸ¤— (from Huggingface Pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import timedelta, datetime\n",
    "import nltk\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.style.use('ggplot')\n",
    "plt.style.use('default')\n",
    "plt.style.use('dark_background')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comments = pd.read_csv('comments.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(548941, 7)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_comments.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1. VADER Seniment Scoring\n",
    "We will use NLTK's SentimentIntensityAnalyzer to get the neg/neu/pos scores of the text.\n",
    "\n",
    "This uses a \"bag of words\" approach:\n",
    "Stop words are removed\n",
    "each word is scored and combined to a total score.\n",
    "\n",
    "Important - This method does not account for the relationship between words, which in human speech is very important, but would give general idea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "sia = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.0, 'neu': 0.318, 'pos': 0.682, 'compound': 0.6468}"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# some examples -- compound score goes from -1 to +1\n",
    "sia.polarity_scores(\"I am so happy!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.468, 'neu': 0.532, 'pos': 0.0, 'compound': -0.6588}"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sia.polarity_scores(\"This is the worst day ever!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Transformer method**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from scipy.special import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using pre-trained roberta-model\n",
    "MODEL = f\"cardiffnlp/twitter-roberta-base-sentiment\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to apply this model on all the comments data in reviews\n",
    "def polarity_scores_roberta(text):\n",
    "    encoded_txt = tokenizer(text, return_tensors='pt')\n",
    "    try:\n",
    "        output = model(**encoded_txt)\n",
    "        scores = output[0][0].detach().numpy()\n",
    "        scores = softmax(scores)\n",
    "        scores_dict = {\n",
    "            'roberta_neg' : scores[0],\n",
    "            'roberta_neu' : scores[1],\n",
    "            'roberta_pos' : scores[2]\n",
    "        }\n",
    "    except:\n",
    "        scores_dict = {}\n",
    "    return scores_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping null comments\n",
    "df_comments.dropna(subset=['clean_comments'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = {}\n",
    "for i, row in df_comments.iterrows():      \n",
    "    try:\n",
    "        text = row['clean_comments']\n",
    "        # Applying vader --> will be useful, when roberta does not work on some comments\n",
    "        vader_result = sia.polarity_scores(text)\n",
    "        vader_result_rename = {}\n",
    "        for key, value in vader_result.items():\n",
    "            vader_result_rename[f\"vader_{key}\"] = value\n",
    "        # Applying roberta\n",
    "        roberta_result = polarity_scores_roberta(text)\n",
    "        both = {'comment': text, **vader_result_rename, **roberta_result}\n",
    "        res[i] = both\n",
    "    except RuntimeError as ex:\n",
    "        print(ex)\n",
    "        print(f'Broke for id {i}')\n",
    "    except Exception as ex:\n",
    "        print(ex)\n",
    "        print(i)\n",
    "\n",
    "pd.DataFrame(res).T.to_csv('comments_with_sentiments.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joining the vader and roberta sentiments scores to the data which contains reviews file other columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(547928, 9)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Making sure the number of rows are same before joining\n",
    "df_comments_sentiments = pd.read_csv('comments_with_sentiments.csv')\n",
    "df_comments_sentiments.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(547928, 7)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_comments.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# By default join happens on indexes\n",
    "df_comments_wsent = df_comments.join(df_comments_sentiments, how='left')[[\n",
    "    'listing_id',\n",
    "    'id',\n",
    "    'date',\n",
    "    'reviewer_id',\n",
    "    'reviewer_name',\n",
    "    'comments',\n",
    "    'clean_comments',\n",
    "    'vader_neg',\n",
    "    'vader_neu',\n",
    "    'vader_pos',\n",
    "    'vader_compound',\n",
    "    'roberta_neg',\n",
    "    'roberta_pos',\n",
    "    'roberta_neu'\n",
    "]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comments_wsent.to_csv('comments_with_sentiments.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "07de844f12b7ce505123c8a0011b27c4c32b306011201dd41064d4acde3c4bc3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
